import requests
from bs4 import BeautifulSoup
import json
import re
import os
from datetime import datetime
import logging
import time
import random

def clean_spaces(value: str) -> str:
    """Replace weird unicode whitespace with a normal space and strip."""
    return re.sub(r'[\u00A0\u202F\u200B]+', ' ', value).strip()

def parse_finn_listing(url: str) -> dict:
    """
    Fetch the HTML of a Finn.no listing and parse both data and metadata.
    """
    print(f"Fetching data from: {url}")

    try:
        response = requests.get(url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")

        # Get the regular listing data
        specs_section = soup.select_one("section.key-info-section dl.emptycheck.columns-2.md\\:columns-3.break-words.mb-0.mt-16")
        if not specs_section:
            print(f"No <dl> with class 'emptycheck ...' found for {url}. Skipping.")
            return None

        listing_data = {}
        div_blocks = specs_section.find_all("div", recursive=False)

        for block in div_blocks:
            dt = block.find("dt")
            dd = block.find("dd")
            if dt and dd:
                key = clean_spaces(dt.get_text(strip=True))
                value = clean_spaces(dd.get_text(strip=True))
                listing_data[key] = value

        # Check status using the same soup object
        status = check_listing_status(soup)
        
        # Get enriched metadata using the same soup object
        # metadata = analyze_listing_content(soup)
        '''if not metadata:
            metadata = {
                "service_historie": "-",
                "Slitedeler_som_bÃ¸r_byttes": "-",
                "Pris_estimat_bytte_av_slitedeler": "-",
                "Bilens_tilstand": "-",
                "Spesifikke_feil": [],  # Empty list for no specific issues
                "Selger": "-",
                "Other_notes": "-"
            }'''

        # Return complete listing object
        return {
            "url": url,
            "data": listing_data,
            # "metadata": metadata,
            "status": status,
            "last_checked": datetime.now().isoformat()
        }

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None

def parse_car_ad(html_content, url):
    soup = BeautifulSoup(html_content, 'html.parser')
    status = check_listing_status(soup)
    
    if status == 'INAKTIV':
        return {'status': 'inactive', 'url': url}
    
    data = {}
    keys = soup.find_all('dt', class_='u-t4')
    values = soup.find_all('dd', class_='u-t4')
    
    for key, value in zip(keys, values):
        key_text = key.get_text(strip=True)
        value_text = value.get_text(strip=True)
        data[key_text] = value_text
    
    return {
        'url': url,
        'data': data,
        'status': 'sold' if status == 'SOLGT' else 'active'
    }

def check_listing_status(soup):
    """
    Check the status of a Finn.no listing
    Returns: 'SOLGT', 'INAKTIV', or 'active'
    """
    status_badge = soup.find('div', class_='bg-[--w-color-badge-warning-background]')
    
    if status_badge:
        text = status_badge.get_text(strip=True).upper()
        if 'SOLGT' in text:
            return 'SOLGT'
        elif 'INAKTIV' in text:
            return 'INAKTIV'
    
    return 'active'

def main(force_reparse=True):  # Always reparse everything
    """
    Main function that reads multiple Finn.no links and updates the database
    """
    # Read links from the file generated by scrape_links.py
    try:
        with open('finn_links_test.json', 'r', encoding='utf-8') as f:
            finn_links = json.load(f)
            logging.info(f"Loaded {len(finn_links)} links from finn_links_test.json")
    except FileNotFoundError:
        logging.error("finn_links_test.json not found. Run scrape_links.py first.")
        raise
    except json.JSONDecodeError:
        logging.error("Error decoding finn_links_test.json")
        raise

    filename = "reactapp/public/finn_listings.json"

    try:
        # Load existing database
        with open(filename, "r", encoding="utf-8") as f:
            database = json.load(f)
            existing_data = database if isinstance(database, list) else database.get('listings', [])
    except (FileNotFoundError, json.JSONDecodeError):
        existing_data = []

    # Create a map of existing listings for easy lookup
    existing_listings = {item["url"]: item for item in existing_data}
    
    # Keep track of processed URLs to know which ones were updated
    processed_urls = set()
    
    # Process all listings from finn_links
    for link in finn_links:
        print(f"\nProcessing: {link}")
        processed_urls.add(link)
        
        # Get the existing listing if available
        existing_listing = existing_listings.get(link, None)
        
        # Try to parse the listing
        listing = parse_finn_listing(link)
    
        if listing:
            # Initialize history array if it doesn't exist
            if not existing_listing:
                print(f"New listing found with status: {listing['status']}")
                listing["status_history"] = [{
                    "status": listing["status"],
                    "date": listing["last_checked"]
                }]
            else:
                print(f"Found existing listing:")
                print(f"  Current status: {listing['status']}")
                print(f"  Previous status: {existing_listing.get('status', 'active')}")
                print(f"  Last checked: {existing_listing.get('last_checked', 'unknown')}")
                
                # Keep history from the existing listing
                listing["status_history"] = existing_listing.get("status_history", [])
                
                # Add new status to history if changed
                if listing["status"] != existing_listing.get("status", "active"):
                    print(f"Status changed for {link}: {existing_listing.get('status', 'active')} -> {listing['status']}")
                    listing["status_history"].append({
                        "status": listing["status"],
                        "date": listing["last_checked"]
                    })
                    if listing["status"] == "sold":
                        listing["sold_date"] = datetime.now().isoformat()
                
                # Preserve any additional fields from the existing listing that aren't in the new one
                for key, value in existing_listing.items():
                    if key not in listing and key != "data" and key != "status":
                        listing[key] = value
            
            # Update the existing_listings map with the new/updated listing
            existing_listings[link] = listing
            print(f"{'Updated' if existing_listing else 'Added'} listing: {link} (Status: {listing['status']})")
        else:
            # If parsing failed, keep the existing data
            if existing_listing:
                print(f"Parsing failed for {link}, keeping existing data")
                # Update last_checked timestamp
                existing_listing["last_checked"] = datetime.now().isoformat()
                existing_listings[link] = existing_listing
            else:
                print(f"Parsing failed for new listing: {link}")
                # Create a placeholder entry for the failed listing
                existing_listings[link] = {
                    "url": link,
                    "status": "parse_error",
                    "last_checked": datetime.now().isoformat(),
                    "status_history": [{
                        "status": "parse_error",
                        "date": datetime.now().isoformat()
                    }]
                }

    print("\nChecking for listings not in current source:")
    # Update listings that weren't in the current batch
    for url, item in existing_listings.items():
        if url not in processed_urls:
            if item.get("status") == "active":
                print(f"Active listing disappeared: {url}")
                item["previous_status"] = item["status"]
                item["status"] = "unknown"
                item["last_checked"] = datetime.now().isoformat()
                
                # Add status change to history
                if "status_history" not in item:
                    item["status_history"] = []
                item["status_history"].append({
                    "status": "unknown",
                    "date": datetime.now().isoformat()
                })
                print(f"Marked as unknown: {url}")
            else:
                print(f"Keeping non-active listing that's not in current source: {url}")
    
    # Convert the map back to a list for saving
    all_listings = list(existing_listings.values())
    
    # Save everything to JSON file
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(all_listings, f, ensure_ascii=False, indent=4)
    
    # Print status breakdown
    statuses = {}
    for item in all_listings:
        status = item.get('status', 'active')
        statuses[status] = statuses.get(status, 0) + 1
    
    print("\nStatus breakdown:")
    for status, count in statuses.items():
        print(f"{status}: {count}")
    print(f"\nTotal listings in database: {len(all_listings)}")

if __name__ == "__main__":
    main()
