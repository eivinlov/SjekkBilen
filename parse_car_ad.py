import requests
from bs4 import BeautifulSoup
import json
import re
import os
from datetime import datetime
import logging
import time
import random

def clean_spaces(value: str) -> str:
    """Replace weird unicode whitespace with a normal space and strip."""
    return re.sub(r'[\u00A0\u202F\u200B]+', ' ', value).strip()

def parse_finn_listing(url: str) -> dict:
    """
    Fetch the HTML of a Finn.no listing and parse both data and metadata.
    """
    print(f"Fetching data from: {url}")

    try:
        response = requests.get(url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")

        # Get the regular listing data
        specs_section = soup.select_one("section.key-info-section dl.emptycheck.columns-2.md\\:columns-3.break-words.mb-0.mt-16")
        if not specs_section:
            print(f"No <dl> with class 'emptycheck ...' found for {url}. Skipping.")
            return None

        listing_data = {}
        div_blocks = specs_section.find_all("div", recursive=False)

        for block in div_blocks:
            dt = block.find("dt")
            dd = block.find("dd")
            if dt and dd:
                key = clean_spaces(dt.get_text(strip=True))
                value = clean_spaces(dd.get_text(strip=True))
                listing_data[key] = value

        # Check status using the same soup object
        status = check_listing_status(soup)
        
        # Get enriched metadata using the same soup object
        # metadata = analyze_listing_content(soup)
        '''if not metadata:
            metadata = {
                "service_historie": "-",
                "Slitedeler_som_bÃ¸r_byttes": "-",
                "Pris_estimat_bytte_av_slitedeler": "-",
                "Bilens_tilstand": "-",
                "Spesifikke_feil": [],  # Empty list for no specific issues
                "Selger": "-",
                "Other_notes": "-"
            }'''

        # Return complete listing object
        return {
            "url": url,
            "data": listing_data,
            # "metadata": metadata,
            "status": status,
            "last_checked": datetime.now().isoformat()
        }

    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None

def parse_car_ad(html_content, url):
    soup = BeautifulSoup(html_content, 'html.parser')
    status = check_listing_status(soup)
    
    if status == 'INAKTIV':
        return {'status': 'inactive', 'url': url}
    
    data = {}
    keys = soup.find_all('dt', class_='u-t4')
    values = soup.find_all('dd', class_='u-t4')
    
    for key, value in zip(keys, values):
        key_text = key.get_text(strip=True)
        value_text = value.get_text(strip=True)
        data[key_text] = value_text
    
    return {
        'url': url,
        'data': data,
        'status': 'sold' if status == 'SOLGT' else 'active'
    }

def check_listing_status(soup):
    """
    Check the status of a Finn.no listing
    Returns: 'SOLGT', 'INAKTIV', or 'active'
    """
    status_badge = soup.find('div', class_='bg-[--w-color-badge-warning-background]')
    
    if status_badge:
        text = status_badge.get_text(strip=True).upper()
        if 'SOLGT' in text:
            return 'SOLGT'
        elif 'INAKTIV' in text:
            return 'INAKTIV'
    
    return 'active'

def main(force_reparse=True):  # Always reparse everything
    """
    Main function that reads multiple Finn.no links and updates the database
    """
    # Read links from the file generated by scrape_links.py
    try:
        with open('finn_links_test.json', 'r', encoding='utf-8') as f:
            finn_links = json.load(f)
            logging.info(f"Loaded {len(finn_links)} links from finn_links_test.json")
    except FileNotFoundError:
        logging.error("finn_links_test.json not found. Run scrape_links.py first.")
        raise
    except json.JSONDecodeError:
        logging.error("Error decoding finn_links_test.json")
        raise

    filename = "reactapp/public/finn_listings.json"

    try:
        # Load existing database
        with open(filename, "r", encoding="utf-8") as f:
            database = json.load(f)
            existing_data = database if isinstance(database, list) else database.get('listings', [])
    except (FileNotFoundError, json.JSONDecodeError):
        existing_data = []

    # Create a map of existing listings for easy lookup
    existing_listings = {item["url"]: item for item in existing_data}
    
    # Process all listings
    updated_results = []
    for link in finn_links:
        print(f"\nProcessing: {link}")
        listing = parse_finn_listing(link)
    
        if listing:
            # Handle status changes
            existing_listing = existing_listings.get(link)
            if existing_listing:
                print(f"Found existing listing:")
                print(f"  Current status: {listing['status']}")
                print(f"  Previous status: {existing_listing.get('status', 'active')}")
                print(f"  Last checked: {existing_listing.get('last_checked', 'unknown')}")
                
                if existing_listing.get("status") == "unknown":
                    print(f"Skipping status update for previously unknown listing: {link}")
                    continue
                    
                if listing["status"] != existing_listing.get("status", "active"):
                    print(f"Status changed for {link}: {existing_listing.get('status', 'active')} -> {listing['status']}")
                    if listing["status"] == "sold":
                        listing["sold_date"] = datetime.now().isoformat()
            else:
                print(f"New listing found with status: {listing['status']}")
            
            if listing["status"] != "inactive":  # Don't add inactive listings
                updated_results.append(listing)
                print(f"{'Updated' if existing_listing else 'Added'} listing: {link} (Status: {listing['status']})")
        else:
            print(f"Skipped inactive listing: {link}")

    print("\nChecking for listings that disappeared:")
    # Mark remaining active listings as unknown
    current_urls = set(finn_links)
    for url, item in existing_listings.items():
        if url not in current_urls and item.get("status") == "active":
            print(f"Listing disappeared: {url}")
            print(f"  Previous status: {item.get('status')}")
            print(f"  Last checked: {item.get('last_checked')}")
            item["status"] = "unknown"
            item["last_checked"] = datetime.now().isoformat()
            updated_results.append(item)
            print(f"Marked as unknown: {url}")
    
    # Save everything to JSON file
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(updated_results, f, ensure_ascii=False, indent=4)
    
    # Print status breakdown
    statuses = {}
    for item in updated_results:
        status = item.get('status', 'active')
        statuses[status] = statuses.get(status, 0) + 1
    
    print("\nStatus breakdown:")
    for status, count in statuses.items():
        print(f"{status}: {count}")
    print(f"\nTotal listings in database: {len(updated_results)}")

if __name__ == "__main__":
    main()
